{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d20801b",
   "metadata": {
    "id": "7d20801b"
   },
   "outputs": [],
   "source": [
    "#!pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a92639c",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a92639c",
    "outputId": "70a81a31-09ab-4c04-f9f0-34515aef0967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Ansar9811291\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Ansar9811291/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py:675\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    674\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Ansar9811291/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reuters\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Download the Reuters dataset if it hasn't been downloaded yet\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Get a list of all document ids in the dataset\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m doc_ids \u001b[38;5;241m=\u001b[39m reuters\u001b[38;5;241m.\u001b[39mfileids()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create a list of dictionaries, where each dictionary represents a document\u001b[39;00m\n\u001b[0;32m     13\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py:675\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    673\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    674\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Ansar9811291/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Ansar9811291\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('reuters')\n",
    "import pandas as pd\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Download the Reuters dataset if it hasn't been downloaded yet\n",
    "\n",
    "# Get a list of all document ids in the dataset\n",
    "doc_ids = reuters.fileids()\n",
    "\n",
    "# Create a list of dictionaries, where each dictionary represents a document\n",
    "documents = []\n",
    "for doc_id in doc_ids:\n",
    "    document = {}\n",
    "    document['id'] = doc_id\n",
    "    document['text'] = reuters.raw(doc_id)\n",
    "    document['categories'] = reuters.categories(doc_id)\n",
    "    documents.append(document)\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "df = pd.DataFrame(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad62600",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cad62600",
    "outputId": "d0bbf03f-3c18-40cb-e39b-0b8efe316e96"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fda91",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb82441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cQMhFTJqZtH8",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cQMhFTJqZtH8"
   },
   "outputs": [],
   "source": [
    "clean_df = df.dropna(axis=0)\n",
    "df=clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f81033",
   "metadata": {},
   "source": [
    "There were no null values in dataset actually. So, it was kind off unnecessary to do remove null code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataset lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66YVsOh5ZtLi",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "66YVsOh5ZtLi"
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7OzOnMt7ZtP_",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7OzOnMt7ZtP_",
    "outputId": "661ab9bd-2849-4ea5-b70e-612e87dc333e"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd076ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tockenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d115cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = df['text']\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_texts = []\n",
    "\n",
    "for i in range(len(df['text'])):\n",
    "    #raw_text = reuters.raw(text[i])\n",
    "    tokens = word_tokenize(df['text'][i])\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    df['text'][i] = filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85p77FXuZtUF",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "85p77FXuZtUF",
    "outputId": "59222783-f157-42b6-bd51-169177cd1d52"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E_GTqeupZtX3",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "E_GTqeupZtX3"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "#text = re.sub('['+string.punctuation+']', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w5PQyYZge2MF",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "w5PQyYZge2MF",
    "outputId": "24daae40-51e9-4551-e124-5a0d453228b8"
   },
   "outputs": [],
   "source": [
    "for i in range(len(df['text'])):\n",
    "    df['text'][i] = re.sub('['+string.punctuation+']', '', df['text'][i])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cCpPqVQme2ZP",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cCpPqVQme2ZP"
   },
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nH_wiTO5e2kD",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nH_wiTO5e2kD",
    "outputId": "6c365933-5409-4323-8e17-ca0e146f0e9a"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(df['text'])):\n",
    "    sentence=df['text'][i]\n",
    "    words = sentence.split()\n",
    "    stemmed_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    stemmed_sentence = ' '.join(stemmed_words)\n",
    "    df['text'][i]=stemmed_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26YHXZHooqs9",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "26YHXZHooqs9",
    "outputId": "c726205e-d60a-427c-ff15-41ec295fc684"
   },
   "outputs": [],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WkqRNgCjorGh",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WkqRNgCjorGh"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(len(df['text'])):\n",
    "    sentence=df['text'][i]\n",
    "    words = sentence.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    stemmed_sentence = ' '.join(stemmed_words)\n",
    "    df['text'][i]=stemmed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EJwWamsOo2cU",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EJwWamsOo2cU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(df['text'])):\n",
    "    string_with_non_alpha = df['text'][i]\n",
    "\n",
    "    # Remove non-alphabetic characters using regex\n",
    "    string_without_non_alpha = re.sub(r'[^a-zA-Z\\s]', '', string_with_non_alpha)\n",
    "    df['text'][i]=string_without_non_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Y-eyoWYo6rH",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7Y-eyoWYo6rH",
    "outputId": "f2d5aeeb-0444-4ff9-cc35-2c33b7f1119b"
   },
   "outputs": [],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eftIhV_po_jP",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "eftIhV_po_jP"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "for i in range(len(df['text'])):\n",
    "    df['text'][i] = remove_html_tags(df['text'][i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SrcaHXTWo_kj",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "SrcaHXTWo_kj",
    "outputId": "31fec23e-8363-4ab9-cfbb-25d6a840000f"
   },
   "outputs": [],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GuKsaTKAo_or",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GuKsaTKAo_or",
    "outputId": "2711583d-b170-49ec-e279-094800ca2b9b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "for i in range(len(df['text'])):\n",
    "    df['text'][i] = remove_urls(df['text'][i])\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v0wzdUzYpOfk",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "v0wzdUzYpOfk",
    "outputId": "ba499b27-8453-4be0-9b13-ac7f41af8908"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oaRh8_f2pOiY",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oaRh8_f2pOiY"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "#data = {'Text': ['John Smith works at Google', 'Mary Johnson lives in New York']}\n",
    "# Create DataFrame\n",
    "#df = pd.DataFrame(data)\n",
    "# Function to extract named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KhyNWR4opOzw",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KhyNWR4opOzw"
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        #print(sentence)\n",
    "        #print('\\n')\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        for chunk in chunks:\n",
    "            entities.append(chunk)\n",
    "\n",
    "            #if isinstance(chunk, Tree):\n",
    "                #entities.append(' '.join([token[0] for token in chunk]))\n",
    "    #print(entities)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZzYvetJop1YU",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzYvetJop1YU",
    "outputId": "cf5aeeff-430d-4db0-e89d-46744c2d7ad1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#print(df['text'][0])\n",
    "#df['NamedEntities'] = extract_entities(df['text'][0])\n",
    "df['NamedEntities'] = df['text'].apply(extract_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nSD7NKx8q-aR",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nSD7NKx8q-aR",
    "outputId": "a00c986d-3beb-4e48-ca08-b7fb84adaa56"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k6gEIk5Op1gJ",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "k6gEIk5Op1gJ"
   },
   "outputs": [],
   "source": [
    "dicto={}\n",
    "for i in range(len(df)):\n",
    "    for item in df['categories'][i]:\n",
    "        if item not in dicto:\n",
    "            dicto[item] = 1\n",
    "        else:\n",
    "            dicto[item] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ORdWD6VQrJUX",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 974
    },
    "id": "ORdWD6VQrJUX",
    "outputId": "3613d4e9-7748-4cb4-dddb-463a8aadfc94"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "key_list=[]\n",
    "val_list=[]\n",
    "for key,value in dicto.items():\n",
    "    key_list.append(key)\n",
    "    val_list.append(value)\n",
    "data = {'category': key_list,\n",
    "        'value': val_list}\n",
    "df_dicto = pd.DataFrame(data)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "# Create a barplot of the data\n",
    "sns.barplot(x='category', y='value', data=df_dicto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NBH2N6-grJV_",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBH2N6-grJV_",
    "outputId": "1cdf6f30-7989-413a-8e72-b9a9c089d6b5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "#df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Create an empty dataframe\n",
    "new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# Loop through each row in the original dataframe\n",
    "for index, row in df.iterrows():\n",
    "    # Get the categories for the current row\n",
    "    categories = row['categories']\n",
    "    # Loop through each category\n",
    "    for category in categories:\n",
    "        # Create a new row with the current category and other data from the original row\n",
    "        new_row = {'id': row['id'], 'text': row['text'], 'categories': [category]}\n",
    "        # Append the new row to the new dataframe\n",
    "        new_df = new_df.append(new_row, ignore_index=True)\n",
    "\n",
    "# Save the new dataframe to a CSV file\n",
    "#new_df.to_csv('new_dataset.csv', index=False)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QQyryPoxrJZ0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QQyryPoxrJZ0",
    "outputId": "4e9eaa57-e912-4e6c-b85e-881d96e5acf5"
   },
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tXUFWOqirRgg",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tXUFWOqirRgg"
   },
   "outputs": [],
   "source": [
    "new_df[\"categories\"] = new_df[\"categories\"].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XHbkRuFbrRja",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XHbkRuFbrRja",
    "outputId": "c9d0af95-0fde-4c98-a457-b8dfb2781590"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Separate features and target\n",
    "X = new_df['text'].values\n",
    "y = new_df['categories'].values\n",
    "\n",
    "# Oversample the minority classes\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_over, y_over = oversample.fit_resample(X.reshape(-1, 1), y)\n",
    "\n",
    "# Undersample the majority class\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_balanced, y_balanced = undersample.fit_resample(X_over, y_over)\n",
    "\n",
    "# Combine balanced X and y into a new dataframe\n",
    "df_balanced = pd.DataFrame({'text': X_balanced.ravel(), 'categories': y_balanced})\n",
    "df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RjHnxMYSrRnt",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "RjHnxMYSrRnt"
   },
   "outputs": [],
   "source": [
    "d = {} \n",
    "for index, row in df_balanced.iterrows():\n",
    "    category = row['categories']\n",
    "    if category in d:\n",
    "        d[category] += 1\n",
    "    else:\n",
    "        d[category] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MpOyEHe9rRvo",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MpOyEHe9rRvo",
    "outputId": "4730e05e-d0c8-4657-ef3f-142a60fed2b5"
   },
   "outputs": [],
   "source": [
    "key_list=[]\n",
    "val_list=[]\n",
    "for key,value in d.items():\n",
    "    key_list.append(key)\n",
    "    val_list.append(value)\n",
    "data = {'category': key_list,\n",
    "        'value': val_list}\n",
    "df_d = pd.DataFrame(data)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "# Create a barplot of the data\n",
    "sns.barplot(x='category', y='value', data=df_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OMJgWPgtrbTj",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OMJgWPgtrbTj",
    "outputId": "308ce05c-84df-4df9-eb3f-5ac6068f5d54"
   },
   "outputs": [],
   "source": [
    "df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z8f1j1KP_3N1",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Z8f1j1KP_3N1",
    "outputId": "633a05b3-1c99-41e6-86cb-867e46201bc2"
   },
   "outputs": [],
   "source": [
    "!pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02t4OJLrApp8",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "02t4OJLrApp8"
   },
   "outputs": [],
   "source": [
    "from summa import keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P1swTW8VAEz1",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "P1swTW8VAEz1",
    "outputId": "5c8bd029-7558-405a-cb8f-67649d34d648"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Automatic summarization is the process of reducing a text document with a \\\n",
    "computer program in order to create a summary that retains the most important points \\\n",
    "of the original document. As the problem of information overload has grown, and as \\\n",
    "the quantity of data has increased, so has interest in automatic summarization. \\\n",
    "Technologies that can make a coherent summary take into account variables such as \\\n",
    "length, writing style and syntax. An example of the use of summarization technology \\\n",
    "is search engines such as Google. Document summarization is another.\"\"\"\n",
    "\n",
    "kw = keywords.keywords(text)\n",
    "print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OWM1pz9DCU8i",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OWM1pz9DCU8i",
    "outputId": "60debaac-a15a-487d-fe4a-e0f5d99f1b87"
   },
   "outputs": [],
   "source": [
    "!pip install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iI5VBAQ2CTae",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "iI5VBAQ2CTae",
    "outputId": "3dcfedae-0d89-46ee-f0b0-2fa7002db966"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yake\n",
    "\n",
    "# Initialize YAKE keyword extractor\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "\n",
    "# Define function to extract keywords from text\n",
    "def get_keywords(text):\n",
    "    # Extract 5 most relevant keywords from text\n",
    "    keywords = kw_extractor.extract_keywords(text)[:5]\n",
    "    return [keyword[0] for keyword in keywords]\n",
    "\n",
    "# Add new column 'keywords' to dataframe\n",
    "df['keywords'] = df['text'].apply(get_keywords)\n",
    "\n",
    "# Print dataframe with added column\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otGF2R7xA7Vh",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "otGF2R7xA7Vh",
    "outputId": "56271bbf-a5be-4f2e-eabf-83073e9ff846"
   },
   "outputs": [],
   "source": [
    "df['keywords'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O3PE6IDMXKE7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3PE6IDMXKE7",
    "outputId": "e327441c-2335-4d7a-c33d-10d8a79a1632"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import yake\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# download required nltk modules\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize YAKE keyword extractor and define TextRank keyword function\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "def get_keyword_rank(text, window_size=2, top_n=10):\n",
    "    # tokenize text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # remove stopwords and punctuation marks\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    filtered_words = []\n",
    "    for sentence in words:\n",
    "        filtered_sentence = []\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        filtered_words.append(filtered_sentence)\n",
    "\n",
    "    # create graph based on co-occurence of words within a window\n",
    "    graph = nx.Graph()\n",
    "    for sentence in filtered_words:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if not graph.has_node(word):\n",
    "                graph.add_node(word)\n",
    "            for j in range(i+1, min(i+window_size, len(sentence))):\n",
    "                other_word = sentence[j]\n",
    "                if not graph.has_node(other_word):\n",
    "                    graph.add_node(other_word)\n",
    "                if not graph.has_edge(word, other_word):\n",
    "                    graph.add_edge(word, other_word, weight=0)\n",
    "                graph[word][other_word]['weight'] += 1\n",
    "\n",
    "    # calculate TextRank scores\n",
    "    scores = nx.pagerank(graph, alpha=0.85)\n",
    "\n",
    "    # sort scores and return top N keywords\n",
    "    ranked_keywords = sorted(((scores[word], word) for word in scores), reverse=True)[:top_n]\n",
    "\n",
    "    return [keyword for score, keyword in ranked_keywords]\n",
    "\n",
    "\n",
    "# Add new column 'keywords' to dataframe\n",
    "df['keywords'] = df['text'].apply(get_keyword_rank)\n",
    "df['yake_keywords'] = df['text'].apply(lambda x: [keyword[0] for keyword in kw_extractor.extract_keywords(x)[:5]])\n",
    "\n",
    "# Print dataframe with added columns\n",
    "print(df)\n",
    "\n",
    "# Build document-keyword graph\n",
    "graph = nx.Graph()\n",
    "for i, row in df.iterrows():\n",
    "    document = row['text']\n",
    "    keywords = row['keywords']\n",
    "    for keyword in keywords:\n",
    "        if not graph.has_node(keyword):\n",
    "            graph.add_node(keyword)\n",
    "        if not graph.has_node(document):\n",
    "            graph.add_node(document)\n",
    "        if not graph.has_edge(document, keyword):\n",
    "            graph.add_edge(document, keyword, weight=0)\n",
    "        graph[document][keyword]['weight'] += 1\n",
    "\n",
    "# Draw graph\n",
    "pos = nx.spring_layout(graph)\n",
    "nx.draw(graph, pos=pos)\n",
    "labels = {node: node for node in graph.nodes()}\n",
    "nx.draw_networkx_labels(graph, pos=pos, labels=labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XynPo7MbMg49",
   "metadata": {
    "id": "XynPo7MbMg49"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import yake\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# download required nltk modules\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize YAKE keyword extractor and define TextRank keyword function\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "def get_keyword_rank(text, top_n=10):\n",
    "    # tokenize text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # remove stopwords and punctuation marks\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    filtered_words = []\n",
    "    for sentence in words:\n",
    "        filtered_sentence = []\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        filtered_words.append(filtered_sentence)\n",
    "\n",
    "    # create graph based on co-occurence of words within a window\n",
    "    graph = nx.Graph()\n",
    "    for sentence in filtered_words:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if not graph.has_node(word):\n",
    "                graph.add_node(word)\n",
    "            for j in range(i+1, min(i+2, len(sentence))):\n",
    "                other_word = sentence[j]\n",
    "                if not graph.has_node(other_word):\n",
    "                    graph.add_node(other_word)\n",
    "                if not graph.has_edge(word, other_word):\n",
    "                    graph.add_edge(word, other_word, weight=0)\n",
    "                graph[word][other_word]['weight'] += 1\n",
    "\n",
    "    # calculate TextRank scores\n",
    "    scores = nx.pagerank(graph, alpha=0.85)\n",
    "\n",
    "    # sort scores and return top N keywords\n",
    "    ranked_keywords = sorted(((scores[word], word) for word in scores), reverse=True)[:top_n]\n",
    "\n",
    "    return [keyword for score, keyword in ranked_keywords]\n",
    "\n",
    "\n",
    "# Add new column 'keywords' to dataframe\n",
    "df['keywords'] = df['text'].apply(get_keyword_rank)\n",
    "df['yake_keywords'] = df['text'].apply(lambda x: [keyword[0] for keyword in kw_extractor.extract_keywords(x)[:5]])\n",
    "\n",
    "# Build document-keyword graph\n",
    "graph = nx.Graph()\n",
    "for i, row in df.iterrows():\n",
    "    document = f\"Doc{i+1}\"  # assign numbered document name to node\n",
    "    keywords = row['keywords']\n",
    "    for j, keyword in enumerate(keywords):\n",
    "        if not graph.has_node(keyword):\n",
    "            graph.add_node(keyword, bipartite=1)  # set keyword nodes as red (bipartite=1)\n",
    "        if not graph.has_node(document):\n",
    "            graph.add_node(document, bipartite=0)  # set document nodes as blue (bipartite=0)\n",
    "        if not graph.has_edge(document, keyword):\n",
    "            graph.add_edge(document, keyword, weight=0)\n",
    "        graph[document][keyword]['weight'] += 1\n",
    "\n",
    "# Separate nodes by bipartite attribute and set node colors\n",
    "top_nodes = {n for n, d in graph.nodes(data=True) if d['bipartite']==0}\n",
    "bottom_nodes = set(graph) - top_nodes\n",
    "node_colors = ['blue' if n in top_nodes else 'red' for n in graph.nodes()]\n",
    "\n",
    "# Draw bipartite graph with node numbers and colors\n",
    "pos = {}\n",
    "pos.update((node, (i, 1)) for i, node in enumerate(sorted(top_nodes)))\n",
    "pos.update((node, (j, 0)) for j, node in enumerate(sorted(bottom_nodes)))\n",
    "nx.draw(graph, pos=pos, with_labels=True, labels={node: i+1 for i, node in enumerate(sorted(graph.nodes()))}, font_size=8, node_color=node_colors)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d65829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select the unique values in the 'NamedEntities' column\n",
    "named_entities = df['NamedEntities'].unique()\n",
    "\n",
    "# get the top 3 or 4 most frequent types of named entities\n",
    "top_entities = df['NamedEntities'].value_counts().head(4).index.tolist()\n",
    "\n",
    "# print the selected named entities\n",
    "print(top_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e0cc7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Select the unique values in the 'NamedEntities' column\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m named_entities \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNamedEntities\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get the top 3 or 4 most frequent types of named entities\u001b[39;00m\n\u001b[0;32m      5\u001b[0m top_entities \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNamedEntities\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Select the unique values in the 'NamedEntities' column\n",
    "named_entities = df['NamedEntities'].unique()\n",
    "\n",
    "# Get the top 3 or 4 most frequent types of named entities\n",
    "top_entities = df['NamedEntities'].value_counts().head(4).index.tolist()\n",
    "\n",
    "# Create a dictionary to store the top entities and their corresponding keywords\n",
    "entity_keywords_dict = {}\n",
    "\n",
    "# Loop through the top entities and extract their keywords using the get_keyword_rank function\n",
    "for entity in top_entities:\n",
    "    # Filter the dataframe to select only the rows containing the current entity\n",
    "    entity_df = df[df['NamedEntities'] == entity]\n",
    "    \n",
    "    # Extract the keywords for the current entity and add them to the dictionary\n",
    "    keywords = []\n",
    "    for text in entity_df['text']:\n",
    "        keywords.extend(get_keyword_rank(text))\n",
    "    entity_keywords_dict[entity] = list(set(keywords))\n",
    "\n",
    "# Add edges between documents, top entities, and keywords\n",
    "for i, row in df.iterrows():\n",
    "    document = f\"Doc{i+1}\"  # assign numbered document name to node\n",
    "    keywords = row['keywords']\n",
    "    for j, keyword in enumerate(keywords):\n",
    "        graph.add_edge(document, keyword, weight=row['yake_keywords'].count(keyword))\n",
    "        if keyword in entity_keywords_dict[row['NamedEntities']]:\n",
    "            graph.add_edge(document, keyword+'_entity', weight=1)\n",
    "    if row['NamedEntities'] in top_entities:\n",
    "        graph.add_edge(document, row['NamedEntities']+'_entity', weight=1)\n",
    "\n",
    "# Separate nodes by bipartite attribute and set node colors\n",
    "top_nodes = {n for n, d in graph.nodes(data=True) if d['bipartite']==0}\n",
    "bottom_nodes = set(graph) - top_nodes\n",
    "node_colors = ['blue' if n in top_nodes else 'red' for n in graph.nodes()]\n",
    "\n",
    "# Draw bipartite graph with node numbers and colors\n",
    "pos = {}\n",
    "pos.update((node, (i, 1)) for i, node in enumerate(sorted(top_nodes)))\n",
    "pos.update((node, (j, 0)) for j, node in enumerate(sorted(bottom_nodes)))\n",
    "nx.draw(graph, pos=pos, with_labels=True, labels={node: i+1 for i, node in enumerate(sorted(graph.nodes()))}, font_size=8, node_color=node_colors)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nodes related to document features (named entities and keywords)\n",
    "document_features = top_entities + list(set([item for sublist in entity_keywords_dict.values() for item in sublist]))\n",
    "\n",
    "# Project bipartite graph onto document features nodes\n",
    "graph_features = bipartite.projected_graph(graph, nodes=document_features, multigraph=False)\n",
    "\n",
    "# Draw the projected graph\n",
    "nx.draw(graph_features, with_labels=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set minimum weight threshold\n",
    "e = 0.5\n",
    "\n",
    "# Get list of edges and their weights\n",
    "edges = list(graph_features.edges())\n",
    "weights = nx.get_edge_attributes(graph_features, 'weight').values()\n",
    "\n",
    "# Remove edges with weight less than e\n",
    "for i in range(len(edges)):\n",
    "    if weights[i] < e:\n",
    "        graph_features.remove_edges_from([edges[i]])\n",
    "        \n",
    "# Draw the pruned graph\n",
    "nx.draw(graph_features, with_labels=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "# Find communities using Louvain algorithm\n",
    "partition = community.best_partition(graph_features)\n",
    "\n",
    "# Get number of nodes in each community\n",
    "community_sizes = {}\n",
    "for node, com in partition.items():\n",
    "    if com not in community_sizes:\n",
    "        community_sizes[com] = 1\n",
    "    else:\n",
    "        community_sizes[com] += 1\n",
    "\n",
    "# Print community sizes\n",
    "print(\"Community sizes:\")\n",
    "print(community_sizes)\n",
    "\n",
    "# Choose a random community to plot\n",
    "import random\n",
    "random_community = random.choice(list(set(partition.values())))\n",
    "\n",
    "# Create subgraph of random community\n",
    "subgraph_nodes = [node for node, com in partition.items() if com == random_community]\n",
    "subgraph = graph_features.subgraph(subgraph_nodes)\n",
    "\n",
    "# Draw the subgraph\n",
    "nx.draw(subgraph, with_labels=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
