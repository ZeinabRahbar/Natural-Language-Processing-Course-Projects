{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0335f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d10c7b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['به',\n",
       " 'ایوان',\n",
       " 'می',\n",
       " 'روم',\n",
       " 'و',\n",
       " 'انگشتانم',\n",
       " 'را',\n",
       " 'بر',\n",
       " 'پوست',\n",
       " 'کشیده',\n",
       " 'ی',\n",
       " 'شب',\n",
       " 'می',\n",
       " 'کشم']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize('به ایوان می روم و انگشتانم را بر پوست کشیده ی شب می کشم')\n",
    "S= text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698b6bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['به',\n",
       " 'ایوان',\n",
       " 'می',\n",
       " 'روم',\n",
       " 'و',\n",
       " 'انگشتانم',\n",
       " 'را',\n",
       " 'بر',\n",
       " 'پوست',\n",
       " 'کشیده',\n",
       " 'ی',\n",
       " 'شب',\n",
       " 'می',\n",
       " 'کشید#کش']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lem = []\n",
    "for i in text:\n",
    "    lem.append(lemmatizer.lemmatize(i))\n",
    "lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6cc5fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['به',\n",
       " 'ایو',\n",
       " 'م',\n",
       " 'رو',\n",
       " 'و',\n",
       " 'انگشتان',\n",
       " 'را',\n",
       " 'بر',\n",
       " 'پوس',\n",
       " 'کشیده',\n",
       " '',\n",
       " 'شب',\n",
       " 'م',\n",
       " 'ک']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "stem = []\n",
    "for i in text:\n",
    "    stem.append(stemmer.stem(i))\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd311bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['پیرانه', 'سرم', 'عشق', 'جوانی', 'به', 'سر', 'افتاد']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'پیرانه سرم عشق جوانی به سر افتاد'\n",
    "S1 = text1\n",
    "\n",
    "text1 = word_tokenize(text1)\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b896d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['پیرانه', 'سر', 'عشق', 'جوان', 'به', 'سر', 'افتاد#افت']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lem = []\n",
    "for i in text1:\n",
    "    lem.append(lemmatizer.lemmatize(i))\n",
    "lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49604fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['پیرانه', 'سر', 'عشق', 'جوان', 'به', 'سر', 'افتاد']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "stem = []\n",
    "for i in text1:\n",
    "    stem.append(stemmer.stem(i))\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3074dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'هری پاتر و زندانی آزکابان جزء غیرقابل پیش بینی ترین داستان های رولینگ است.'\n",
    "S2 = text2\n",
    "\n",
    "text2 = word_tokenize(text2)\n",
    "tags = nltk.pos_tag(text2)\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "tree = parser.parse(tags)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ccb6061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['لالا', '،', 'لالا', '،', 'گل', 'خشخاش', '،', 'بابات', 'رفته', 'خدا', 'همراش']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'لالا، لالا، گل خشخاش، بابات رفته خدا همراش'\n",
    "S5 = text2\n",
    "text2 = word_tokenize(text2)\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d012c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    " normalizer = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f61f422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['لالا', '،', 'لالا', '،', 'گل', 'خشخاش', '،', 'بابات', 'رفته', 'خدا', 'همراش']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = []\n",
    "for i in text2:\n",
    "    norm.append(normalizer.normalize(i))\n",
    "norm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ab48c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = 'عجب دارم از ان زلف چلیپا'\n",
    "S3 = text3\n",
    "\n",
    "text3 = word_tokenize(text3)\n",
    "tags = nltk.pos_tag(text3)\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "tree = parser.parse(tags)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5c59ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = 'آفتاب گرم و دل چسبی که تمام پیش از ظهر بر شهر زیبای کرمانشاه نور افشانده بود با سماجتی هرچه افزون تر می کوشید تا آخرین اثر برف شب پیش را از میان بردارد'\n",
    "S4 = text4\n",
    "text4 = word_tokenize(text4)\n",
    "tags = nltk.pos_tag(text4)\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "parser = nltk.RegexpParser(grammar)\n",
    "tree = parser.parse(tags)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192fba0",
   "metadata": {},
   "source": [
    "# plot wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9fe7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud_fa import WordCloudFa\n",
    "\n",
    "wc = WordCloudFa(width=1200, height=800)\n",
    "\n",
    "Sentenses = [str(S), str(S1), str(S2), str(S3), str(S4), str(S5)]\n",
    "\n",
    "for i in Sentenses:\n",
    "    word_cloud = wc.generate(i)\n",
    "    image = word_cloud.to_image()\n",
    "    image.show()\n",
    "    image.save('persian-example.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
